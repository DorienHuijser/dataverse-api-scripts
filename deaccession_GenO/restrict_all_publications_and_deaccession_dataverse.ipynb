{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a08759-d80d-4e3d-93a8-730ece3dca52",
   "metadata": {},
   "source": [
    "# Restrict all publications in a Dataverse and Deaccession V1\n",
    "\n",
    "- Date of creation: 2025-03-31 by Dorien Huijser\n",
    "- Date of last edit: 2025-04-11\n",
    "\n",
    "## Background\n",
    "\n",
    "All publications in the GenO Archive Dataverse (<https://dataverse.nl/dataverse/GenO_Archive>) need to be set to restricted access in a newer version, and the previous version deaccessioned, due to potential copyright issues.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Admin access to a dataverse\n",
    "- Python installation\n",
    "\n",
    "## ⚠️ Current issues with this script ⚠️\n",
    "\n",
    "⚠️ At the moment it is not yet possible in the DataverseNL instance of Harvard Dataverse to restrict files via the API - or at least I have not found a solution to do so. The function `restrict_file` in this notebook therefore does not work; you have to indicate the terms of use/that you want to enable access requests, which is not possible via the API (but will be, I think: <https://github.com/IQSS/dataverse/pull/11349>).\n",
    "\n",
    "## Structure of this document\n",
    "\n",
    "- Import libraries\n",
    "- Find all DOIs and dataset IDs of the GenO Archive dataverse, and put them together in a pandas dataframe (function `get_dois_and_ids`)\n",
    "- Look for the file ID(s) per dataset and add them to the pandas dataframe (function `retrieve_file_ids`)\n",
    "- ⚠️ Set all files to restricted access > Does not work (yet?) (function `restrict_file`)\n",
    "- Change the publication date to the Journal publication date (function `update_citation_date`)\n",
    "- Publish as new version of the dataset (function `publish_dataset`)\n",
    "- Deaccession version 1 of each dataset (function `deaccession_dataset`)\n",
    " \n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9645d-68f6-4468-9286-4b37f8b9e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd              # For working with dataframes\n",
    "import requests                  # For connecting with the Dataverse API\n",
    "import json                      # For deaccessioning the dataset\n",
    "import os                        # For reading in the api token txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464f59a-aaaf-419c-b178-c428a60ac71a",
   "metadata": {},
   "source": [
    "## Get all DOIs and dataset IDs from in the dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e294a-5d5f-4738-8fb0-dd76974d4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dois_and_ids(base_url, dv_parent_alias, api_token):\n",
    "    '''Get a dataframe of DOIs and dataset IDs from the datasets in the specified Dataverse collection'''\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "    \n",
    "    # List dataverse contents\n",
    "    request = requests.get(f'{base_url}/api/dataverses/{dv_parent_alias}/contents', headers = headers)\n",
    "    response_data = request.json()\n",
    "    \n",
    "    # Extract the list of persistent identifiers\n",
    "    persistent_urls = [\n",
    "        item['persistentUrl'].replace('https://doi.org/', 'doi:') \n",
    "        for item in response_data['data']\n",
    "    ]\n",
    "\n",
    "    # Extract the file ids\n",
    "    ds_ids = [item['id'] for item in response_data['data']]\n",
    "\n",
    "    # Put both in a pandas dataframe\n",
    "    result = pd.DataFrame({'persistent_urls': persistent_urls, \n",
    "                           'ds_ids': ds_ids})\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f04954-07f9-4fb6-b0f5-718051232317",
   "metadata": {},
   "source": [
    "## Get the file IDs per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07f62d-e2a9-433e-b079-8d842ea1ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the file ids for each DOI in the dataverse\n",
    "def retrieve_file_ids(persistent_urls, base_url, api_token):\n",
    "    '''Get the corresponding file ids from a list of Dataverse persistent identifiers'''\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "\n",
    "    # Create an empty pandas dataframe that will be filled\n",
    "    doi_files = pd.DataFrame({\"persistent_urls\": [],\n",
    "                             \"file_ids\": []})\n",
    "    \n",
    "    for doi in persistent_urls:\n",
    "        file_ids = []\n",
    "        request = requests.get(f'{base_url}/api/datasets/:persistentId/?persistentId={doi}', headers = headers)\n",
    "        response_data = request.json()\n",
    "\n",
    "        # Retrieve all the file IDs for this DOI\n",
    "        for field in response_data['data']['latestVersion']['files']:\n",
    "            if field[\"dataFile\"][\"id\"]:\n",
    "                the_id = str(field[\"dataFile\"][\"id\"])\n",
    "            # Make a list of all the file IDs per DOI\n",
    "            file_ids.append(the_id)\n",
    "\n",
    "        # Add the file IDs to the corresponding DOI in the doi_files dataframe\n",
    "        tempdf = pd.DataFrame({'persistent_urls': doi, 'file_ids': [file_ids]})\n",
    "        doi_files = pd.concat([doi_files, tempdf], ignore_index=True)\n",
    "        \n",
    "    return(doi_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e483ea4-6de3-42fa-b14b-3a1a02882b8c",
   "metadata": {},
   "source": [
    "## Restrict the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42496a42-f4bc-4734-b923-609b8152047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does not work yet:\n",
    "# {\"status\":\"ERROR\",\"message\":\"Terms of Use and Access are invalid. You must enable request access or add terms of access in datasets with restricted files.\"}\n",
    "\n",
    "def restrict_file(fileid, base_url, api_token):\n",
    "    '''Restrict specified files in a Dataverse upload. This creates a new draft of the dataset.'''\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "\n",
    "    data = 'true'\n",
    "    \n",
    "    response = requests.put(f'{base_url}/api/files/{fileid}/restrict', headers = headers, data = data)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041da82-d4ec-481f-baaa-1e22331364bd",
   "metadata": {},
   "source": [
    "## Change the publication date to the journal publication date\n",
    "\n",
    "By default Dataverse uses the date of uploading/publishing as the citation date. We want this to be the date that the journal article was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a53cf9-ed82-43e3-b7ce-c763d024d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_citation_date(pid, base_url, api_token):\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "\n",
    "    payload = 'journalPubDate' # Take the field journalPubDate as citation date\n",
    "    \n",
    "    # Making the PUT request with the correct URL and payload\n",
    "    response = requests.put(\n",
    "        f'{base_url}/api/datasets/:persistentId/citationdate?persistentId={pid}',\n",
    "        headers = headers, \n",
    "        data = payload\n",
    "    )\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83901cc8-0fd7-410d-b662-bea537271dcd",
   "metadata": {},
   "source": [
    "## Publish the dataset in a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb0fe3-9e46-4e1a-9675-924c110b5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_dataset(pid, base_url, api_token, major_or_minor):\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    } \n",
    "\n",
    "    response = requests.post(\n",
    "        f'{base_url}/api/datasets/:persistentId/actions/:publish?persistentId={pid}&type={major_or_minor}',\n",
    "        headers = headers)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b90c3-bc77-4611-a531-8909d23b55fb",
   "metadata": {},
   "source": [
    "## Deaccession the old version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b06413-4118-4fb8-86ac-921aeb34def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deaccession_dataset(ds_id, version, api_token, base_url):\n",
    "    # Define the deaccession reason and other details\n",
    "    deaccession_data = {\n",
    "        \"deaccessionReason\": \"File restricted due to copyright, see v2.0 of this dataset\"\n",
    "        # \"deaccessionForwardURL\": \"https://demo.dataverse.nl\"\n",
    "    }\n",
    "    \n",
    "    # Convert the deaccession data to JSON format\n",
    "    json_data = json.dumps(deaccession_data)\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token,\n",
    "        'Content-Type': 'application/json'  # This is for the overall request, but will be handled by 'files'\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'{base_url}/api/datasets/{ds_id}/versions/{version}/deaccession', \n",
    "                             headers = headers, \n",
    "                             data = json_data)\n",
    "\n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb0cf2-472d-46c3-b2bb-755b2aaab4a3",
   "metadata": {},
   "source": [
    "## Execute all created functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcb8d4-19f8-4aec-b9ef-071b1c7c9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataverse API token from a dv_api_token.txt file in my local parent folder\n",
    "parent_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Read the API token from the file\n",
    "with open(os.path.join(parent_folder, 'dv_api_token.txt'), 'r') as file:\n",
    "    api_token = file.read().strip()\n",
    "\n",
    "base_url = \"https://dataverse.nl\"\n",
    "dv_parent_alias = \"GenO_Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df859b-f811-4f35-8ef4-1b266ba4e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Retrieve all DOIs, dataset IDs and file IDs\n",
    "all_datasets = get_dois_and_ids(base_url, dv_parent_alias, api_token)\n",
    "fileidsdf = retrieve_file_ids(all_datasets['persistent_urls'], base_url, api_token)\n",
    "\n",
    "all_datasets_inclfiles = all_datasets.merge(fileidsdf, how = 'outer')\n",
    "print(all_datasets_inclfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505faadd-3a6d-4220-8c84-af5bfe083727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to perform for each dataset:\n",
    "# NB This used to be 1 giant for loop, but because the restrict_file function does not work, I've split them up in separate loops so that they can be separately run\n",
    "\n",
    "# Step 2: Restrict all files of this dataset (if there is/are file ids)\n",
    "for i, doi in enumerate(all_datasets_inclfiles['persistent_urls']):\n",
    "    print(f'Attempting to restrict dataset {i}: {doi}')\n",
    "    \n",
    "    # NOTE: THIS CODE IS NOT EXECUTED NOW\n",
    "    if not pd.isna(all_datasets_inclfiles['file_ids'].iloc[i]).any(): # ADDED .any()\n",
    "        print(\"   Dataset does have file IDs\")\n",
    "        for file in all_datasets_inclfiles['file_ids'].iloc[i]:\n",
    "            print(f'   Restricting file: {file}')\n",
    "            restrict_file(file, base_url, api_token)\n",
    "    else:\n",
    "        print(f\"No files present in dataset {doi}, skipping to the next dataset\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf109a0-3644-4412-bcf1-31fdac35fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Update citation date to journal publication date\n",
    "for i, doi in enumerate(all_datasets_inclfiles['persistent_urls']):\n",
    "    print(f'Updating citation of dataset {i}: {doi}')\n",
    "    update_citation_date(doi, base_url, api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f5856-2298-41f8-900c-95e99adefa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Publish a new version of this dataset\n",
    "for i, doi in enumerate(all_datasets_inclfiles['persistent_urls']):\n",
    "    print(f'Publishing a new version of dataset {i}: {doi}')\n",
    "    publish_dataset(doi, base_url, api_token, 'major')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a2d9c-b068-4a7d-9c94-851dce5a0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Deaccession v1.0\n",
    "for i, doi in enumerate(all_datasets_inclfiles['persistent_urls']):\n",
    "    print(f'Deaccessioning dataset {i}: {doi}')\n",
    "    deaccession_dataset(all_datasets_inclfiles['ds_ids'].iloc[i], \"1.0\", api_token, base_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
