{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a08759-d80d-4e3d-93a8-730ece3dca52",
   "metadata": {},
   "source": [
    "# Upload pdf files to DataverseNL\n",
    "\n",
    "- Date of creation: 2025-03-25 by Dorien Huijser\n",
    "- Date of last edit: 2025-04-11\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "In February 2025, a researcher from the faculty of Social and Behavioural Sciences needed help with uploading over a hundred pdf files to DataverseNL.\n",
    "Many articles from the Dutch journal \"Gedrag en Organisatie\" (in between its inception and 2004) were never published online. \n",
    "The PsychInfo database does however have a bunch of metadata available from some of these journal articles, and the researcher had a local archive of the pdf files (the papers) from  the journal. Although it is not preventable that some papers would have to be published manually (and providing manual metadata), the journals for which there is PsychInfo metadata available *can* be uploaded via a script.\n",
    "\n",
    "In this file, the Python code can be found that uses the [Dataverse Native API](https://guides.dataverse.org/en/latest/api/native-api.html) to upload all papers to DataverseNL for which:\n",
    "\n",
    "- There is a pdf file in the researcher's local `archief` folder\n",
    "- There is metadata from the PsychInfo database\n",
    "- There is no DOI (which means it already is available online) and the publication date is before 2004\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- a folder called 'Archief' that contains all the pdf files to be uploaded. The Archief folder is not included in this repository, but there is a txt file that gives the folder tree (`sourcefiles/GenO_dataverse_foldertree.txt`).\n",
    "- a `citation.xlsx` file, which is a PsychInfo export file. This folder is not included in this repository (due to possible copyright issues), but the column labels are included in the file `sourcefiles/citation_labels.csv`.\n",
    "- a json file with dataverse metadata fields to be filled. Here this is `dataverse_metadata/metadata_GO_v1.json`\n",
    "- admin (or at least write) access to the specified Dataverse collection, and thus the API token and Dataverse ID of that collection.\n",
    "- a Python installation\n",
    "\n",
    "## Structure of this document\n",
    "\n",
    "- Import libraries\n",
    "- Read in psychinfo export file (`citation.xlsx`) in a pandas dataframe\n",
    "- For every pdf in the Archief folder, get the authors, year, volume and issue and put that in a pandas dataframe (`extract_info` function) > result: `results/01_extracted_path_information_archief_folder.csv`\n",
    "- Merge the psychinfo export and the Archief results to see which files overlap: select only those that are present in both (meaning that there is a pdf file (from Archief folder) and sufficient metadata (from the Psychinfo export)) so that they can be uploaded to DataverseNL. > result: `results/02_full_merge_archief_and_psychinfo.csv` and subsets (`2a`, `2b`, `2c`).\n",
    "- For each row in the merged dataframe for which there is both a pdf and psychinfo metadata:\n",
    "  - Fill the Dataverse metadata JSON (function `create_dv_metadata`)\n",
    "  - Check if a dataset with the same name has already been uploaded, skip that one then (functions `get_dataverse_dois`, `retrieve_titles`)\n",
    "  - Create a draft Dataverse dataset with the metadata and retrieve its PID (function `create_dataset`) \n",
    "  - Change the publication date to the journal publication date (otherwise every entry gets 2025 as citation date; function `update_citation_date`)\n",
    "  - Upload the accompanying pdf file to the dataset using the PID (function `upload_data`) > result: `results/03_Files_uploaded_to_Dataverse.csv`\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9645d-68f6-4468-9286-4b37f8b9e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path         # For working with paths\n",
    "import glob                      # For search/matching values with regular expressions\n",
    "import pandas as pd              # For working with dataframes\n",
    "import copy                      # To create copies of objects\n",
    "from datetime import datetime    # To work with dates\n",
    "import requests                  # For connecting with the Dataverse API\n",
    "import json                      # To work with JSON files and dictionaries\n",
    "import os                        # For working with paths\n",
    "import re                        # For using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f3365-30d5-4073-83ee-9447b98d878c",
   "metadata": {},
   "source": [
    "## Read in the PsychInfo export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2b171-08c6-402c-bb5a-f616e2cd24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in excel file - the first row is empty\n",
    "psychinfo = pd.read_excel(\"sourcefiles/citation.xlsx\", skiprows = [0])\n",
    "\n",
    "# Get only the papers without a DOI and before 2004 - these papers do not have to be uploaded\n",
    "psychinfo_sel = psychinfo.loc[(psychinfo['DO'].isnull()) & (psychinfo['YR'] < 2004)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae54ba2-e88f-4512-a83e-3e0e3712c6d3",
   "metadata": {},
   "source": [
    "## Retrieve information from the files and paths in the archief directory\n",
    "\n",
    "Information to retrieve from the path names includes:\n",
    "\n",
    "- YR (year)\n",
    "- Author last names\n",
    "- IP (issue)\n",
    "- VO (jaargang/volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c116d85-549b-4a1f-9e11-ce7f43133e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths for all pdf files in the archief directory\n",
    "archive_directory = Path.cwd().joinpath(\"archief\")\n",
    "archive_pdfs = list(archive_directory.rglob(\"*.pdf\"))\n",
    "relative_paths = [pdf.relative_to(archive_directory) for pdf in archive_pdfs]\n",
    "\n",
    "# Turn paths into a dataframe with 1 column (more columns to come)\n",
    "path_infos = pd.DataFrame(data = {\"path\": relative_paths})\n",
    "\n",
    "# Function to extract information from the path\n",
    "def extract_info(path):\n",
    "    # Split the path to get the relevant parts\n",
    "    path_obj = Path(path)\n",
    "    \n",
    "    # Extract the relevant parts\n",
    "    year_volume = path_obj.parts[0]  # e.g., \"1997 (jaargang 10)\"\n",
    "    year_match = re.search(r'(\\d{4})', year_volume)\n",
    "    volume_match = re.search(r'jaargang (\\d+)', year_volume)\n",
    "    \n",
    "    year = year_match.group(1) if year_match else None\n",
    "    volume = volume_match.group(1) if volume_match else None\n",
    "    \n",
    "    # Extract the issue from the second part\n",
    "    issue_match = re.search(r'GO_(\\d+)_(\\d+)', path_obj.parts[1])\n",
    "    issue = issue_match.group(2) if issue_match else None\n",
    "    \n",
    "    # Extract authors from the filename with a RegEx\n",
    "    filename = path_obj.name  # Get the filename from the path\n",
    "\n",
    "    # Take into account that in some cases there is a \"_\" before the author name and sometimes it is a \" \".\n",
    "    # Also, sometimes there is 4+5 in the issue number (instead of a single number) and 1 file starts with 'gO' instead of \"GO\" \n",
    "    match = re.search(r\"(?i)GO_\\d{4}_\\d+(?:\\+\\d+)?[ _](.+)\", filename, re.IGNORECASE)\n",
    "    if match:\n",
    "    # Split authors by comma and strip whitespace\n",
    "        author_string = re.sub(r\"\\.pdf$\", \"\", match.group(1)) # Get rid of the .pdf extension\n",
    "        authors = [author.strip() for author in author_string.split(\",\")]\n",
    "    \n",
    "    return pd.Series([path, volume, issue, year, authors])\n",
    "\n",
    "# Apply the function to the path_infos DataFrame\n",
    "for i, row in enumerate(path_infos['path']):\n",
    "    result = extract_info(row)\n",
    "    path_infos.loc[i, ['path', 'volume', 'issue', 'year', 'authors']] = result.values\n",
    "\n",
    "# Write the resulting DataFrame to a csv file\n",
    "Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "path_infos.to_csv(\"results/01_extracted_path_information_archief_folder.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a68072-2389-4470-a62f-c2752319a090",
   "metadata": {},
   "source": [
    "## Retrieve papers which have both metadata (psychinfo) and the accompanying file (archief) \n",
    "\n",
    "Some papers are not in the psychinfo export, but are in the archief directory. And others are in the psychinfo export, but are not in the archief directory. We only want the entries that are both in the psychinfo export as well as having an accompanying pdf file in the archief directory, because otherwise there is no file to be uploaded or not enough metadata to upload the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39905ab4-1b83-4dc9-af4a-fbf796ff12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to the same data type\n",
    "def int_to_string(df):\n",
    "    int_columns = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    \n",
    "    if len(int_columns) > 0:\n",
    "        print(f\"Int columns found, converting to string: {int_columns}\")\n",
    "        # Convert object columns to string type\n",
    "        df[int_columns] = df[int_columns].astype('string')\n",
    "\n",
    "int_to_string(psychinfo_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a63380-dfc4-408f-8de6-a36689bc1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the authors columns to be merged\n",
    "# The paths in the archief directory only contain last names, while the authors in the psychinfo export also contain first names or initials\n",
    "def extract_last_names(author_str):\n",
    "    '''get only the last names from the psychinfo export ('AU' column).'''\n",
    "    authors = author_str.split(\"\\n\\n\")  # Split into list\n",
    "    authors = [a.lstrip(\"* \").split(\",\")[0] for a in authors]  # Remove '*' and extract last name\n",
    "    return tuple(authors)  # Convert to tuple (hashable)\n",
    "\n",
    "psychinfo_sel = psychinfo_sel.copy()  # Avoid SettingWithCopyWarning\n",
    "psychinfo_sel.loc[:, \"authors\"] = psychinfo_sel[\"AU\"].apply(extract_last_names)\n",
    "\n",
    "# Convert path_infos['authors'] to tuples\n",
    "path_infos = path_infos.copy()  # Avoid SettingWithCopyWarning\n",
    "path_infos.loc[:, \"authors\"] = path_infos[\"authors\"].apply(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233aba-5a6f-4bab-85db-dacc589a9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for merging\n",
    "psychinfo_sel = psychinfo_sel.rename(columns={\"IP\": \"issue\", \"VO\": \"volume\", \"YR\": \"year\"})\n",
    "\n",
    "# Merge DataFrames on authors, issue, volume, and year\n",
    "fullmerge = psychinfo_sel.merge(path_infos, on=[\"authors\", \"issue\", \"volume\", \"year\"], how=\"outer\", indicator = \"merge_result\")\n",
    "\n",
    "# Recode the merge_result column\n",
    "rename_merge_result = {\"left_only\" : \"Present in Psychinfo, not in archief\", \n",
    "                       \"right_only\" : \"Present in archief, not in Psychinfo\",\n",
    "                       \"both\" : \"Can be uploaded to DataverseNL\"}\n",
    "\n",
    "fullmerge['merge_result'] = fullmerge['merge_result'].map(rename_merge_result)\n",
    "\n",
    "# Sort fullmerge on merge_result\n",
    "fullmerge = fullmerge.sort_values(by=['merge_result'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4b335-d6cc-420f-a2c3-2d666e1dfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets for easier handling/processing and write them to CSV\n",
    "fullmerge.to_csv(\"results/02_full_merge_archief_and_psychinfo.csv\")\n",
    "\n",
    "# Write datasets that can be uploaded to dataverse\n",
    "dataverseready = fullmerge[fullmerge['merge_result'] == \"Can be uploaded to DataverseNL\"]\n",
    "dataverseready.to_csv(\"results/02a_matches_can_be_uploaded_to_dataverse.csv\")\n",
    "\n",
    "# Write datasets that are only in Psychinfo\n",
    "psychinfo_only = fullmerge[fullmerge['merge_result'] == \"Present in Psychinfo, not in archief\"]\n",
    "psychinfo_only.to_csv(\"results/02b_papers_in_psychinfo_not_in_archief.csv\")\n",
    "\n",
    "# Write datasets that are only in the Archief folder\n",
    "archief_only = fullmerge[fullmerge['merge_result'] == \"Present in archief, not in Psychinfo\"]\n",
    "archief_only.to_csv(\"results/02c_papers_in_archief_not_in_psychinfo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529083f4-e894-4f96-a945-a061412ed390",
   "metadata": {},
   "source": [
    "## Create Dataverse metadata JSON\n",
    "\n",
    "- license name: \"CC-BY-NC-ND-4.0\" \n",
    "- license uri: \"http://creativecommons.org/licenses/by-nc-nd/4.0\"\n",
    "- title: OT\n",
    "- alternativeTitle: TI\n",
    "- journalVolumeIssue > journalPubDate = YR\n",
    "- author: AU (!! NB every cell contains multiple authors separated by \\n)\n",
    "- notesText = AB\n",
    "- Journal = JN (fieled not available in Dataverse)\n",
    "- journalVolumeIssue > journalVolume = VO\n",
    "- journalVolumeIssue > journalIssue = IP\n",
    "- datasetContact > datasetContactEmail: \"G&O@uu.nl\"\n",
    "- datasetContact > datasetContactName: \"Gedrag & Organisatie\"\n",
    "- datasetContact > datasetContactAffiliation: \"Utrecht University\"\n",
    "- dsDescription > dsDescriptionValue: \"[TYPE OF ARTICLE] + Gedrag & Organisatie. Gedrag & Organisatie, Tijdschrift voor Sociale, Arbeids- & Organisatiepsychologie, is een wetenschappelijk tijdschrift voor de Nederlandse en Vlaamse markt. Naast wetenschappelijk onderzoek (gebaseerd op kwantitatief en kwalitatief onderzoek) publiceert Gedrag & Organisatie o.a. theoretische uiteenzettingen en overzichtsartikelen.\"\n",
    "- dsDescription > dsDescriptionDate: [same date as date of publication]\n",
    "- Subject: Social Sciences\n",
    "- depositor: Brenninkmeijer, Veerle\n",
    "- dateOfDeposit: (current date)\n",
    "- language: LG\n",
    "- otherIdAgency: \"Tijdschrift voor Sociale, Arbeids- & Organisatiepsychologie\"\n",
    "- otherIdValue: \"Gedrag & Organisatie\"\n",
    "- keywords: ID\n",
    "- topic classification: MH\n",
    "- other references: RF\n",
    "- universe: PO\n",
    "- country: LO and if empty: \"Netherlands\"\n",
    "- Article type: MD. Unique values that have to be translated to Dataverse controlled vocabulary\n",
    "   - Empirical Study; Quantitative Study\n",
    "   - Empirical Study\n",
    "   - Empirical Study; Experimental Replication; Longitudinal Study; Quantitative Study\n",
    "   - nan\n",
    "   - Literature Review\n",
    "   - Empirical Study; Longitudinal Study\n",
    "   - Empirical Study; Qualitative Study\n",
    "   - Empirical Study; Nonclinical Case Study\n",
    "   - Empirical Study; Followup Study; Longitudinal Study; Prospective Study\n",
    "   - Empirical Study; Followup Study\n",
    "   - Empirical Study; Experimental Replication\n",
    "   - Empirical Study; Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae44c0e-c86d-4f9a-acc9-c0b2f52e0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recode the MD column into Dataverse vocabulary for the field articleType\n",
    "def recode_md(md_value):\n",
    "    '''Recode the MD (methods) column to the Dataverse controlled vocabulary for articleType'''\n",
    "    #Possibilities in Dataverse: abstract, addendum, announcement, article-commentary, book review, books received, brief report, calendar, case report, collection, correction, data paper, discussion, dissertation, editorial, in brief, introduction, letter, meeting report, news, obituary, oration, partial retraction, product review, rapid communication, reply, reprint, research article, retraction, review article, translation, other\n",
    "    if not pd.isna(md_value):\n",
    "        if \"Empirical Study\" in md_value: return(\"research article\")\n",
    "        elif \"Literature Review\" in md_value: return(\"review article\")\n",
    "        else: return(\"research article\")\n",
    "    else: return(\"research article\")\n",
    "\n",
    "# Function to recode the LO column into Dataverse vocabulary for the field country\n",
    "def recode_lo(lo_value):\n",
    "    '''Recode the LO (location) column to the Dataverse controlled vocabulary for country'''\n",
    "    # If the input is a list of values: create a new list as output\n",
    "    if isinstance(lo_value, list):\n",
    "        new_lo_value = []\n",
    "        for listitem in lo_value:\n",
    "            listitem = listitem.strip() # Remove leading and trailing spaces (e.g., \"Netherlands, US\" would turn into \"Netherlands\" and \" US\")\n",
    "            if listitem == \"US\": new_lo_value.append(\"United States\")\n",
    "            elif listitem == 'nan' or listitem == '': new_lo_value.append('Netherlands')\n",
    "            else: new_lo_value.append(listitem)\n",
    "        return(new_lo_value)\n",
    "    \n",
    "    # If the input is nan\n",
    "    elif pd.isna(lo_value): return(\"Netherlands\")\n",
    "    \n",
    "    # Recode empty values to Netherlands\n",
    "    elif lo_value == '': return('Netherlands')\n",
    "    \n",
    "    # Other cases\n",
    "    else: return(lo_value.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbe461-cd3a-4dc0-9b98-da7d35389271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create metadata file\n",
    "def create_dv_metadata(row):\n",
    "    '''Create a metadata JSON file that complies with the Dataverse metadata fields.\n",
    "    Input: a row of the dataframe `dataverseready`\n",
    "    Output: a json_data object'''\n",
    "    \n",
    "    # Open and read the Gedrag & Organisatie metadata file (a file with fewer fields than the complete because not all dataverse fields will be used)\n",
    "    with open('dataverse_metadata/metadata_GO_v1.json', 'r') as file:\n",
    "        jsonfile = json.load(file)\n",
    "    \n",
    "    json_data = copy.deepcopy(jsonfile)\n",
    "    \n",
    "    ## Get metadata to fill it in the json_data\n",
    "    # Metadata constant across datasets\n",
    "    description = \"Gedrag & Organisatie. \\nGedrag & Organisatie, Tijdschrift voor Sociale, Arbeids- & Organisatiepsychologie, is een wetenschappelijk tijdschrift voor de Nederlandse en Vlaamse markt. Naast wetenschappelijk onderzoek (gebaseerd op kwantitatief en kwalitatief onderzoek) publiceert Gedrag & Organisatie o.a. theoretische uiteenzettingen en overzichtsartikelen.\"\n",
    "    descriptiondate = datetime.today().strftime('%Y-%m-%d')\n",
    "    license_name = \"CC-BY-NC-ND-4.0\" # NB controlled vocabulary\n",
    "    license_uri = \"http://creativecommons.org/licenses/by-nc-nd/4.0\" # NB controlled vocabulary\n",
    "    subject = [\"Social Sciences\"] # NB controlled vocabulary\n",
    "    contactname = [\"Gedrag & Organisatie\"]\n",
    "    contactemail = [\"G&O@uu.nl\"]\n",
    "    contactaffiliation = [\"Utrecht University\"]\n",
    "    depositor = \"Brenninkmeijer, Veerle\"\n",
    "    depositdate = datetime.today().strftime('%Y-%m-%d')\n",
    "    otheridagency = 'Tijdschrift voor Sociale, Arbeids- & Organisatiepsychologie'\n",
    "    otheridvalue = 'Gedrag & Organisatie'\n",
    "    kindofdata = ['written text']\n",
    "    \n",
    "    # Metadata that change per paper/dataset\n",
    "    title = row['OT'] if pd.notna(row['OT']) else row['TI'] # sometimes OT is empty and then we take the TI column instead\n",
    "    author = row['AU'].split(\"\\n\\n\")\n",
    "    alttitle = [row['TI']]\n",
    "    pubdate = str(row['year'])\n",
    "    notes = row['AB']\n",
    "    volume = str(row['volume'])\n",
    "    issue = str(row['issue'])\n",
    "    country = recode_lo(str(row['LO']).split(\",\")) # NB controlled vocabulary\n",
    "    universe = [row['PO']]\n",
    "    otherrefs = row['RF'].split(\"\\n\\n\") if pd.notna(row['RF']) else []\n",
    "    topicclassification = [term.replace('*', '').strip() for term in row['MH'].split(\"\\n\\n\")]\n",
    "    keywords = str(row['ID']).split(\",\")\n",
    "    language = str(row['LG']).split(\",\") # NB controlled vocabulary\n",
    "    articletype = recode_md(row['MD']) # NB  controlled vocabulary\n",
    "    keywords = [term.strip() for term in row['ID'].split(',')]\n",
    "\n",
    "    ## Store the metadata into the json metadata format\n",
    "    json_data['datasetVersion']['license']['name'] = license_name\n",
    "    json_data['datasetVersion']['license']['uri'] = license_uri\n",
    "\n",
    "    # Citation metadata\n",
    "    fields_citation = json_data[\"datasetVersion\"][\"metadataBlocks\"][\"citation\"][\"fields\"]\n",
    "    \n",
    "    for field in fields_citation:\n",
    "        # Title\n",
    "        if field[\"typeName\"] == \"title\":\n",
    "            field[\"value\"] = title\n",
    "\n",
    "        # Alternative title (English title)\n",
    "        if field[\"typeName\"] == \"alternativeTitle\":\n",
    "            field[\"value\"] = alttitle\n",
    "\n",
    "        # Author: has to deal with multiple authors\n",
    "        if field[\"typeName\"] == \"author\":\n",
    "        \n",
    "            # Access the value list within author\n",
    "            existing_authors = field[\"value\"]\n",
    "            num_existing_authors = len(existing_authors)\n",
    "            \n",
    "            # Update existing authors\n",
    "            for i in range(min(num_existing_authors, len(author))):\n",
    "                existing_authors[i][\"authorName\"][\"value\"] = author[i]\n",
    "                \n",
    "            # Add new authors if there are more in the authors list\n",
    "            for i in range(num_existing_authors, len(author)):\n",
    "                new_author = {\n",
    "                    \"authorName\": {\n",
    "                        \"typeName\": \"authorName\",\n",
    "                        \"multiple\": False,\n",
    "                        \"typeClass\": \"primitive\",\n",
    "                        \"value\": author[i]\n",
    "                    }\n",
    "                }\n",
    "                existing_authors.append(new_author)  # Append new author\n",
    "\n",
    "        # Description\n",
    "        if field['typeName'] == \"dsDescription\":\n",
    "            for descfield in field[\"value\"]:\n",
    "                if \"dsDescriptionValue\" in descfield:\n",
    "                    descfield[\"dsDescriptionValue\"][\"value\"] = description\n",
    "                if \"dsDescriptionDate\" in descfield:\n",
    "                    descfield['dsDescriptionDate']['value'] = descriptiondate\n",
    "\n",
    "        # Subject\n",
    "        if field[\"typeName\"] == \"subject\":\n",
    "            field[\"value\"] = subject\n",
    "\n",
    "        # Dataset contact\n",
    "        if field['typeName'] == 'datasetContact':\n",
    "            # Access the value list within author\n",
    "            for i, cnt in enumerate(field[\"value\"]):\n",
    "                if i < len(cnt):  # Ensure we don't go out of bounds\n",
    "                    cnt[\"datasetContactName\"][\"value\"] = contactname[i]\n",
    "                    cnt[\"datasetContactEmail\"][\"value\"] = contactemail[i]\n",
    "                    cnt[\"datasetContactAffiliation\"][\"value\"] = contactaffiliation[i]\n",
    "\n",
    "         # Other Id Agency + Value\n",
    "        if field['typeName'] == 'otherId':\n",
    "            for otherid in field[\"value\"]:\n",
    "                otherid[\"otherIdAgency\"][\"value\"] = otheridagency\n",
    "                otherid[\"otherIdValue\"][\"value\"] = otheridvalue\n",
    "\n",
    "        # Notes (abstract)\n",
    "        if field[\"typeName\"] == \"notesText\":\n",
    "            field[\"value\"] = notes\n",
    "\n",
    "        # Language\n",
    "        if field['typeName'] == 'language':\n",
    "            field['value'] = language\n",
    "\n",
    "        # Depositor + deposit date\n",
    "        if field['typeName'] == 'depositor':\n",
    "            field['value'] = depositor\n",
    "\n",
    "        if field['typeName'] == 'dateOfDeposit':\n",
    "            field['value'] = depositdate\n",
    "\n",
    "        # other references\n",
    "        if field['typeName'] == 'otherReferences':\n",
    "            field['value'] = otherrefs\n",
    "\n",
    "        # Topic classification\n",
    "        if field['typeName'] == 'topicClassification':\n",
    "            existing_topics = field['value']\n",
    "            num_existing_topics = len(existing_topics)\n",
    "\n",
    "            # Update existing topic field\n",
    "            for i in range(min(num_existing_topics, len(topicclassification))):\n",
    "                existing_topics[i]['topicClassValue']['value'] = topicclassification[i]\n",
    "\n",
    "            # Add new topic classifications if there are more in the topicclassification object\n",
    "            for i in range(num_existing_topics, len(topicclassification)):\n",
    "                new_topic = {\n",
    "                    \"topicClassValue\": {\n",
    "                      \"typeName\": \"topicClassValue\",\n",
    "                      \"multiple\": False,\n",
    "                      \"typeClass\": \"primitive\",\n",
    "                      \"value\": topicclassification[i]\n",
    "                    }\n",
    "                }\n",
    "                existing_topics.append(new_topic) # append new topic\n",
    "\n",
    "        # Keywords\n",
    "        if field['typeName'] == 'keyword':\n",
    "            existing_keywords = field['value']\n",
    "            num_existing_keywords = len(existing_keywords)\n",
    "\n",
    "            # Update existing keyword field\n",
    "            for i in range(min(num_existing_keywords, len(keywords))):\n",
    "                existing_keywords[i]['keywordValue']['value'] = keywords[i]\n",
    "\n",
    "            # Add new keyword if there are more in the keywords list\n",
    "            for i in range(num_existing_keywords, len(keywords)):\n",
    "                new_keyword = {\n",
    "                    \"keywordValue\": {\n",
    "                      \"typeName\": \"keywordValue\",\n",
    "                      \"multiple\": False,\n",
    "                      \"typeClass\": \"primitive\",\n",
    "                      \"value\": keywords[i]\n",
    "                    }\n",
    "                }\n",
    "                existing_keywords.append(new_keyword) # append new keyword\n",
    "\n",
    "        # Kind of data\n",
    "        if field['typeName'] == 'kindOfData':\n",
    "            field['value'] = kindofdata\n",
    "\n",
    "    # Geospatial metadata\n",
    "    fields_geo = json_data[\"datasetVersion\"][\"metadataBlocks\"][\"geospatial\"][\"fields\"]\n",
    "\n",
    "    for field in fields_geo:\n",
    "        # Country\n",
    "        if field['typeName'] == 'geographicCoverage':\n",
    "        #for geo in field['value']:\n",
    "        #    geo['country']['value'] = country\n",
    "            existing_covfields = field[\"value\"]\n",
    "            num_countries = len(existing_covfields)\n",
    "\n",
    "            # Update existing countries\n",
    "            for i in range(min(num_countries, len(country))):\n",
    "                existing_covfields[i]['country']['value'] = country[i]\n",
    "\n",
    "            # Add new countries if there are more in the country list\n",
    "            for i in range(num_countries, len(country)):\n",
    "                new_country = {\n",
    "                    \"country\": {\n",
    "                        \"typeName\": \"country\",\n",
    "                        \"multiple\": False,\n",
    "                        \"typeClass\": \"controlledVocabulary\",\n",
    "                        \"value\": country[i]\n",
    "                    }\n",
    "                }\n",
    "                existing_covfields.append(new_country) # Append new country\n",
    "\n",
    "    # Social sciences and humanities metadata\n",
    "    fields_social = json_data[\"datasetVersion\"][\"metadataBlocks\"][\"socialscience\"][\"fields\"]\n",
    "    for field in fields_social:\n",
    "        if field['typeName'] == 'universe':\n",
    "            field['value'] = universe\n",
    "\n",
    "    # Journal metadata\n",
    "    fields_journal = json_data[\"datasetVersion\"][\"metadataBlocks\"][\"journal\"][\"fields\"]\n",
    "\n",
    "    for field in fields_journal:\n",
    "    \n",
    "        # Issue, volume, publication date\n",
    "        if field['typeName'] == 'journalVolumeIssue':\n",
    "            for vol_is in field['value']:\n",
    "                vol_is['journalVolume']['value'] = volume\n",
    "                vol_is['journalIssue']['value'] = issue\n",
    "                vol_is['journalPubDate']['value'] = pubdate\n",
    "\n",
    "        # Article type\n",
    "        if field['typeName'] == 'journalArticleType':\n",
    "            field['value'] = articletype\n",
    "\n",
    "    # Return the modified JSON\n",
    "    #print(\"Here is the resulting Json file:\\n\\n\", json.dumps(json_data, indent=2))\n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464f59a-aaaf-419c-b178-c428a60ac71a",
   "metadata": {},
   "source": [
    "## Check if a dataset has already been created with the same title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e294a-5d5f-4738-8fb0-dd76974d4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the paper has already been uploaded: we won't want duplicates!\n",
    "def get_dataverse_dois(base_url, dv_parent_alias, api_token):\n",
    "    '''Get a list of DOIs from the datasets in the specified Dataverse collection'''\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "    \n",
    "    # List dataverse contents\n",
    "    request = requests.get('%s/api/dataverses/%s/contents' % (base_url, dv_parent_alias), headers = headers)\n",
    "    response_data = request.json()\n",
    "    \n",
    "    # Extract the list of persistent identifiers\n",
    "    persistent_urls = [\n",
    "        item['persistentUrl'].replace('https://doi.org/', 'doi:') \n",
    "        for item in response_data['data']\n",
    "    ]\n",
    "\n",
    "    return(persistent_urls)\n",
    "\n",
    "# Retrieve the titles for each DOI in the dataverse\n",
    "def retrieve_titles(persistent_urls, base_url, api_token):\n",
    "    '''Get the corresponding titles from a list of Dataverse persistent identifiers'''\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "    \n",
    "    titles = []\n",
    "    for doi in persistent_urls:\n",
    "        request = requests.get('%s/api/datasets/:persistentId/?persistentId=%s' % (base_url, doi), headers = headers)\n",
    "        response_data = request.json()\n",
    "        \n",
    "        for field in response_data['data']['latestVersion']['metadataBlocks']['citation']['fields']:\n",
    "            if field[\"typeName\"] == \"title\":\n",
    "                the_title = field[\"value\"]\n",
    "        titles.append(the_title)\n",
    "        \n",
    "    return(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dff7f8-19f3-406c-817b-b173decbf8c2",
   "metadata": {},
   "source": [
    "## Create a dataset with the metadata in Dataverse and retrieve its PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1870351-598e-4ab8-abec-fb4712fb4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a draft dataset using the Dataverse Native API\n",
    "def create_dataset(json_data, base_url, api_token, dv_parent_alias):    \n",
    "    # Prepare the headers\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token,\n",
    "        'Content-Type': 'application/json'  # Set the content type\n",
    "    }\n",
    "    \n",
    "    # Post request\n",
    "    response = requests.post(f'{base_url}/api/dataverses/{dv_parent_alias}/datasets', \n",
    "                             headers = headers, \n",
    "                             data = json.dumps(json_data))\n",
    "    \n",
    "    # Check the response\n",
    "    if response.status_code == 201:\n",
    "        # Retrieve DOI (persistent identifier / pid)\n",
    "        pid = response.json()['data']['persistentId']\n",
    "        return(pid)\n",
    "    else:\n",
    "        print(f\"Failed to create dataset: {response.status_code}, {response.text}\")\n",
    "        #print(\"\\nDataset:\", json_data)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041da82-d4ec-481f-baaa-1e22331364bd",
   "metadata": {},
   "source": [
    "## Change the publication date to the journal publication date\n",
    "\n",
    "By default Dataverse uses the date of uploading/publishing as the citation date. We want this to be the date that the journal article was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a53cf9-ed82-43e3-b7ce-c763d024d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_citation_date(pid, base_url, api_token):\n",
    "    headers = {\n",
    "        'X-Dataverse-key': api_token\n",
    "    }\n",
    "\n",
    "    payload = 'journalPubDate'\n",
    "    \n",
    "    # Making the PUT request with the correct URL and payload\n",
    "    response = requests.put(\n",
    "        f'{base_url}/api/datasets/:persistentId/citationdate?persistentId={pid}',\n",
    "        headers = headers, \n",
    "        data = payload\n",
    "    )\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eed398-48d9-4afe-9663-1a6a9a9cb4ab",
   "metadata": {},
   "source": [
    "## Upload the corresponding file to Dataverse\n",
    "\n",
    "Using the PID, upload the corresponding file to Dataverse as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01edd45b-d4e1-424d-9e4f-cbda01d19d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(pid, file_path, base_url, api_token):\n",
    "    # Get the full file path to be uploaded to DataverseNL\n",
    "    path = Path(os.path.join(\"archief\", file_path))\n",
    "\n",
    "    # Restrict access\n",
    "    params = {\"restrict\":\"false\"} #dict(description = title)\n",
    "    params_as_json_string = json.dumps(params)\n",
    "    payload = dict(jsonData = params_as_json_string)\n",
    "    \n",
    "    # Code based on: https://guides.dataverse.org/en/latest/api/native-api.html#add-file-api\n",
    "    # Open the file in binary mode\n",
    "    with open(path, 'rb') as file:\n",
    "        files = {'file': (path.name, file)}  # pass the filename and the opened file\n",
    "\n",
    "        # Add file using the Dataset's persistentId\n",
    "        url_persistent_id = '%s/api/datasets/:persistentId/add?persistentId=%s&key=%s' % (base_url, pid, api_token)\n",
    "        r = requests.post(url_persistent_id, \n",
    "                          data = payload, \n",
    "                          files = files)\n",
    "\n",
    "        # Check the response\n",
    "        if r.status_code == 200:\n",
    "            print(\"Successfully uploaded file: \", file_path)\n",
    "        else:\n",
    "            print(\"Failed to upload file: \", r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33e820-54a5-4ef2-8387-631ecfc5ca3f",
   "metadata": {},
   "source": [
    "## Perform all the steps using 1 master function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6c2df-e5f8-4673-8ab1-1474ca136bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umbrella function to apply the 3 dataverse-related functions to the dataverseready dataframe\n",
    "def process_dataframe(df, base_url, dv_parent_alias, api_token):\n",
    "    '''Master function that performs all substeps in a row:\n",
    "    1. Create a new column called pid that will store the newly created DOIs\n",
    "    Then, for every row in the provided dataframe:\n",
    "    2. Create Dataverse-compliant metadata\n",
    "    3. Check if a dataset with the same name is already present in the provided Dataverse collection\n",
    "    4. If not: Create a dataverse dataset using the metadata\n",
    "    5. Update the citation date to match the journal publication date (otherwise the citation date will be the date of upload)\n",
    "    6. Upload the corresponding data file\n",
    "    '''\n",
    "    \n",
    "    # Create a new column for pids in the dataframe\n",
    "    df['pid'] = None\n",
    "\n",
    "    # Prep for checking if a dataset has already been uploaded\n",
    "    dois = get_dataverse_dois(base_url, dv_parent_alias, api_token)\n",
    "    titles = retrieve_titles(dois, base_url, api_token)\n",
    "\n",
    "    # Perform the steps per row in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        #print(\"row index: \", index)\n",
    "        \n",
    "        # Step 1: Create metadata JSON\n",
    "        json_data = create_dv_metadata(row)\n",
    "\n",
    "        # Step 2: Check if a dataset with the same title has already been created\n",
    "        title = row['OT'] if pd.notna(row['OT']) else row['TI']\n",
    "        \n",
    "        if not title in titles: # Only if there is no dataset with an existing title\n",
    "        \n",
    "            # Step 3: Create dataset and get the PID\n",
    "            pid = create_dataset(json_data, base_url, api_token, dv_parent_alias)\n",
    "            \n",
    "            if pid:  # if the dataset was successfully created\n",
    "                # Step 4: Update the citation date to the journal publication date\n",
    "                update_citation_date(pid, base_url, api_token)\n",
    "                \n",
    "                # Step 5: Upload the data file\n",
    "                upload_data(pid, row['path'], base_url, api_token)\n",
    "        else:\n",
    "            print(\"There is already a dataset with the same name in this Dataverse collection, skipping this row\")\n",
    "            print(\"Index: \", index, \"  Title: \", title)\n",
    "            pid = \"Already present in Dataverse\"\n",
    "    \n",
    "        # Save the pid at the correct index in the dataframe\n",
    "        df.at[index, 'pid'] = pid\n",
    "    print(\"Done uploading to Dataverse\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcb8d4-19f8-4aec-b9ef-071b1c7c9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataverse API token from a dv_api_token.txt file in my local parent folder\n",
    "parent_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Read the API token from the file\n",
    "with open(os.path.join(parent_folder, 'dv_api_token.txt'), 'r') as file:\n",
    "    api_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7ca93-9b58-495e-94b5-0f3a73937355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all the Dataverse-related functions\n",
    "base_url =  \"https://dataverse.nl\"                    # Base URL of your Dataverse instance, without trailing slash (e. g. https://data.aussda.at))\n",
    "#api_token = \"********-****-****-****-************\"   # API token of a Dataverse user with proper rights to create a Dataset (DO NOT SHARE)\n",
    "dv_parent_alias = \"GenO_Archive\"                      # Alias of the Dataverse, the Dataset should be attached to.\n",
    "\n",
    "completed_df = process_dataframe(dataverseready, base_url, dv_parent_alias, api_token)\n",
    "\n",
    "# Write to CSV\n",
    "completed_df.to_csv(\"results/03_Files_uploaded_to_Dataverse.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
